import numpy as np
import scipy.stats as stats
import scipy.signal as signal
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import time
import sys
import warnings

warnings.filterwarnings('ignore')


def waitforEnter():
    if sys.version_info[0] == 2:
        raw_input("Press ENTER to continue.")
    else:
        input("Press ENTER to continue.")


def hellingerDist(p, q):
    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / np.sqrt(2)


plt.ion()
# -1- #
data1 = np.loadtxt('../dataFiles/data1')
# for dataset 0 (from 0 to 39)
plt.figure(1)
plt.plot(data1[:, 0], marker='s', c='blue', label='dataset 0')
plt.show()

# -2- #
M = np.mean(data1, axis=0)
Md = np.median(data1, axis=0)
V = np.var(data1, axis=0)
S = stats.skew(data1)
K = stats.kurtosis(data1)
p = range(5, 101, 1)
Pr = np.percentile(data1, p, axis=0)

plt.figure(2)
plt.subplot(5, 1, 1)
plt.plot(M)
plt.ylabel('mean')
plt.subplot(5, 1, 2)
plt.plot(Md)
plt.ylabel('median')
plt.subplot(5, 1, 3)
plt.plot(V)
plt.ylabel('variance')
plt.subplot(5, 1, 4)
plt.plot(S)
plt.ylabel('skewness')
plt.subplot(5, 1, 5)
plt.plot(K)
plt.ylabel('kurtosis')
plt.show()

# -4- #
# for dataset 0 (from 0 to 39)
pdf, bins = np.histogram(data1[:, 0], bins=50, density=True)
dbin = np.diff(bins)[0]
cdf = np.cumsum(pdf) * dbin
x = bins[:-1]
plt.figure(3)
plt.plot(x, pdf, marker='s', c='blue', label='dataset 0 PDF')
plt.show()
plt.figure(4)
plt.plot(x, cdf, marker='s', c='blue', label='dataset 0 CDF')
plt.show()

# -5- #
# Q-Q plot for dataset 0 and dataset 1
plt.figure(4)
plt.clf()
p = range(5, 101, 1)
Pr0 = np.percentile(data1[:, 0], p)
Pr1 = np.percentile(data1[:, 1], p)
plt.scatter(Pr0, Pr1, marker='o', c='blue')
lp = [0, max(Pr0[-1], Pr1[-1])]
plt.plot(lp, lp, c='red')
plt.show()
# P-P plot for dataset 0 and dataset 1
plt.figure(5)
plt.clf()
pdf0, bins = np.histogram(data1[:, 0], bins=50, density=True)
dbin = np.diff(bins)[0]
cdf0 = np.cumsum(pdf0) * dbin
# bins/xvalues of the CDF should be the same
pdf1, bins = np.histogram(data1[:, 1], bins=bins, density=True)
dbin = np.diff(bins)[0]
cdf1 = np.cumsum(pdf1) * dbin
plt.scatter(cdf0, cdf1, marker='o', c='blue')
lp = [min(cdf0[0], cdf1[0]), 1]
plt.plot(lp, lp, c='red')
plt.show()

# -7- #
# for dataset 0
stats.kstest(data1[:, 0], 'expon')
stats.kstest(data1[:, 0], 'norm')
# for dataset 0 and dataset 1
stats.ks_2samp(data1[:, 0], data1[:, 1])
# for dataset 0 and dataset 30
stats.ks_2samp(data1[:, 0], data1[:, 30])

# -8- #
data2 = np.loadtxt('data2')
plt.figure(6)
pdf, x, y = np.histogram2d(data1[:, 0], data2[:, 0], bins=10)
xx, yy = np.meshgrid(x, y)
plt.pcolormesh(xx, yy, pdf)
plt.show()

# -9- #
plt.figure(7)
data1All = np.loadtxt('data1All')
for a in range(20, 501, 20):
    plt.clf()
    Agg = np.sum(data1All[:, 0:a], axis=1)
    pdf, x = np.histogram(Agg, bins=20, density=True)
    m = np.mean(Agg)
    std = np.std(Agg)  # standard deviation = sqrt( variance )
    plt.plot(x[:-1], pdf, 'k', label='empirical PDF (' + str(a) + ' users)')
    plt.plot(x, mlab.normpdf(x, m, std), 'r', label='inferred Gaussian PDF')
    plt.show()
    plt.legend()
    waitforEnter()

# -10- #
plt.figure(8)
traff = np.loadtxt('traff')
C = abs(np.corrcoef(traff, rowvar=0))
plt.pcolormesh(C)
plt.show()

# -11- #
# for dataset 2
plt.figure(9)
x = data1[:, 2]
lag = np.arange(0, 100, 1)
xcorr = np.zeros(100)
xcorr[0] = np.correlate(x, x)
for l in lag[1:]:
    xcorr[l] = np.correlate(x[:-l], x[l:])
plt.plot(lag, xcorr)
plt.show()

# -12- #
# for dataset 2 (with modulus-squared of FFT)
plt.figure(10)
x = data1[:, 2]
fft = np.fft.fft(x)
psd = abs(fft) ** 2
plt.plot(psd[:50])
plt.show()
# for dataset 2 (with Welch's method )
f, psd = signal.periodogram(x)
plt.plot(1 / f[:50], psd[:50])
plt.show()

# -13- #
import scalogram

x = data1[:, 2]
scales = np.arange(1, 50)
plt.ion()
plt.figure(11)
cwt = scalogram.CWTfft(x, scales)
plt.imshow(abs(cwt), cmap=plt.cm.Blues, aspect='auto')
plt.show()
plt.figure(12)
S, scales = scalogram.scalogramCWT(x, scales)
plt.plot(scales, S)
plt.show()

# -14- #
# features
M1 = np.mean(data1, axis=0)
Md1 = np.median(data1, axis=0)
V1 = np.var(data1, axis=0)
S1 = stats.skew(data1)
K1 = stats.kurtosis(data1)
p = [25, 50, 75, 90, 95]
Pr1 = np.array(np.percentile(data1, p, axis=0)).T
M2 = np.mean(data2, axis=0)
Md2 = np.median(data2, axis=0)
V2 = np.var(data2, axis=0)
S2 = stats.skew(data2)
K2 = stats.kurtosis(data2)
Pr2 = np.array(np.percentile(data2, p, axis=0)).T
features = np.c_[M1, M2, Md1, Md2, V1, V2, S1, S2, K1, K2, Pr1, Pr2]
# PCA
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
rcp = pca.fit(features).transform(features)
plt.figure(13)
plt.scatter(rcp[0:19, 0], rcp[0:19, 1], marker='o', c='r', label='datasets 0-19')
plt.scatter(rcp[20:, 0], rcp[20:, 1], marker='s', c='b', label='datasets 20-39')
plt.legend()
plt.show()

# -15- #
from sklearn.cluster import KMeans

rcp = PCA(n_components=2).fit_transform(features)
# K-means assuming 2 clusters
kmeans = KMeans(init='k-means++', n_clusters=2)
kmeans.fit(rcp)
kmeans.labels_
# vizualization plot
x_min, x_max = 1.5 * rcp[:, 0].min(), 1.5 * rcp[:, 0].max()
y_min, y_max = 1.5 * rcp[:, 1].min(), 1.5 * rcp[:, 1].max()
N = 20
hx = (x_max - x_min) / N
hy = (y_max - y_min) / N
xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure(14)
plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Blues,
           aspect='auto', origin='lower', alpha=0.7)
plt.plot(rcp[:, 0], rcp[:, 1], 'ko')
# Plot the centroids as a white X
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='r')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.show()

# -16- #
from sklearn.cluster import DBSCAN

rcp = PCA(n_components=2).fit_transform(features)
# DBSCAN assuming a neighborhood maximum distance of 1e11
dbscan = DBSCAN(eps=1e11)
dbscan.fit(rcp)
L = dbscan.labels_
print(L)
colors = plt.cm.Blues(np.linspace(0, 1, len(set(L))))
plt.figure(15)
for l in set(L):
    p = (L == l)
    if l == -1:
        color = 'r'
    else:
        color = colors[l]
    plt.plot(rcp[p, 0], rcp[p, 1], 'o', c=color, markersize=10)
plt.show()

# -17- #
from sklearn.covariance import EllipticEnvelope

anom_perc = 20
clf = EllipticEnvelope(contamination=.1)
clf.fit(rcp)
clf.decision_function(rcp).ravel()
pred = clf.decision_function(rcp).ravel()
threshold = stats.scoreatpercentile(pred, anom_perc)
Anom = pred > threshold
print(Anom)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure(16)
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7), cmap=plt.cm.Blues_r)
plt.contour(xx, yy, Z, levels=[threshold], linewidths=2, colors='red')
plt.plot(rcp[:, 0], rcp[:, 1], 'ko')
plt.show()

# -18- #
x = data1[:, 2]
M = np.mean(x)
V = np.var(x)
H, bins = np.histogram(x, bins=20)
probs = 1.0 * H / np.sum(H)
# Exponential
ye = np.random.exponential(M, (300, 20))
# Gaussian/Normal
yg = np.random.normal(M, V ** 0.5, (300, 20))
# Empirical discrete
yd = np.random.choice(bins[:-1], (300, 20), p=probs)

# -20- #
import json
from scipy.optimize import curve_fit

with open('ams-ix-traffic.json') as data_file:
    data = json.load(data_file)
Xout = []
for monthT in data:
    Xout.append(monthT['out_traffic'])


def linearG(t, Y0, R):
    return Y0 + R * t


def expG(t, Y0, A0, R):
    return Y0 + A0 * np.exp(R * t)


t = np.arange(0, len(Xout))
paramsL, cov = curve_fit(linearG, t, Xout)
paramsE, cov = curve_fit(expG, t, Xout, [500, 1, .01])
plt.figure(17)
plt.plot(t, Xout, 'k')
plt.plot(t, linearG(t, paramsL[0], paramsL[1]), 'b')
plt.plot(t, expG(t, paramsE[0], paramsE[1], paramsE[2]), 'r')
plt.show()

# End
waitforEnter()
